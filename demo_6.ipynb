{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('xor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"x1\", \"x2\"]].values\n",
    "y = df[\"class label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.15, random_state=1, stratify=y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.1, random_state=1, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class PytorchMLP(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.linear_1 = torch.nn.Linear(num_features, 25)    \n",
    "        self.linear_2 = torch.nn.Linear(25, 15)\n",
    "        self.linear_3 = torch.nn.Linear(15, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = F.relu(x)\n",
    "        x= self.linear_2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear_3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "class XORData(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features= torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "    \n",
    "\n",
    "train_dataset = XORData(X_train, y_train)\n",
    "val_dataset = XORData(X_val, y_val)\n",
    "test_dataset = XORData(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validatation_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_accuracy(model, dataloader):\n",
    "\n",
    "    model = model.eval()\n",
    "    \n",
    "    correct = 0.0\n",
    "    total_examples = 0\n",
    "    \n",
    "    for idx, (features, labels) in enumerate(dataloader):\n",
    "        \n",
    "        with torch.inference_mode(): # basically the same as torch.no_grad\n",
    "            logits = model(features)\n",
    "        \n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        compare = labels == predictions\n",
    "        correct += torch.sum(compare)\n",
    "        total_examples += len(compare)\n",
    "\n",
    "    return correct / total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch index: 0, Loss: 0.675772488117218\n",
      "Epoch: 0, Batch index: 1, Loss: 0.6912350654602051\n",
      "Epoch: 0, Batch index: 2, Loss: 0.6793786883354187\n",
      "Epoch: 0, Batch index: 3, Loss: 0.6760478615760803\n",
      "Epoch: 0, Batch index: 4, Loss: 0.6922560930252075\n",
      "Epoch: 0, Batch index: 5, Loss: 0.6893620491027832\n",
      "Epoch: 0, Batch index: 6, Loss: 0.6598774790763855\n",
      "Epoch: 0, Batch index: 7, Loss: 0.6838384866714478\n",
      "Epoch: 0, Batch index: 8, Loss: 0.6826114654541016\n",
      "Epoch: 0, Batch index: 9, Loss: 0.690500795841217\n",
      "Epoch: 0, Batch index: 10, Loss: 0.6667635440826416\n",
      "Epoch: 0, Batch index: 11, Loss: 0.659843385219574\n",
      "Epoch: 0, Batch index: 12, Loss: 0.682585597038269\n",
      "Epoch: 0, Batch index: 13, Loss: 0.6836134195327759\n",
      "Epoch: 0, Batch index: 14, Loss: 0.659523069858551\n",
      "Epoch: 0, Batch index: 15, Loss: 0.6781588792800903\n",
      "Epoch: 0, Batch index: 16, Loss: 0.6774669289588928\n",
      "Epoch: 0, Batch index: 17, Loss: 0.6672713160514832\n",
      "Epoch: 0, Train accuracy: 0.5794066190719604, Validation accuracy: 0.5625\n",
      "Epoch: 1, Batch index: 0, Loss: 0.6619376540184021\n",
      "Epoch: 1, Batch index: 1, Loss: 0.6860165596008301\n",
      "Epoch: 1, Batch index: 2, Loss: 0.6815198659896851\n",
      "Epoch: 1, Batch index: 3, Loss: 0.6679577231407166\n",
      "Epoch: 1, Batch index: 4, Loss: 0.6742618680000305\n",
      "Epoch: 1, Batch index: 5, Loss: 0.6925328373908997\n",
      "Epoch: 1, Batch index: 6, Loss: 0.6695086359977722\n",
      "Epoch: 1, Batch index: 7, Loss: 0.6656590700149536\n",
      "Epoch: 1, Batch index: 8, Loss: 0.6730501651763916\n",
      "Epoch: 1, Batch index: 9, Loss: 0.6732650399208069\n",
      "Epoch: 1, Batch index: 10, Loss: 0.6649438142776489\n",
      "Epoch: 1, Batch index: 11, Loss: 0.6860342621803284\n",
      "Epoch: 1, Batch index: 12, Loss: 0.6626257300376892\n",
      "Epoch: 1, Batch index: 13, Loss: 0.6857946515083313\n",
      "Epoch: 1, Batch index: 14, Loss: 0.6790653467178345\n",
      "Epoch: 1, Batch index: 15, Loss: 0.6663954854011536\n",
      "Epoch: 1, Batch index: 16, Loss: 0.6736351847648621\n",
      "Epoch: 1, Batch index: 17, Loss: 0.6545539498329163\n",
      "Epoch: 1, Train accuracy: 0.6247818470001221, Validation accuracy: 0.59375\n",
      "Epoch: 2, Batch index: 0, Loss: 0.6708178520202637\n",
      "Epoch: 2, Batch index: 1, Loss: 0.6820371150970459\n",
      "Epoch: 2, Batch index: 2, Loss: 0.6799464821815491\n",
      "Epoch: 2, Batch index: 3, Loss: 0.6691069006919861\n",
      "Epoch: 2, Batch index: 4, Loss: 0.6615305542945862\n",
      "Epoch: 2, Batch index: 5, Loss: 0.6734978556632996\n",
      "Epoch: 2, Batch index: 6, Loss: 0.6651929020881653\n",
      "Epoch: 2, Batch index: 7, Loss: 0.6844388842582703\n",
      "Epoch: 2, Batch index: 8, Loss: 0.6681746244430542\n",
      "Epoch: 2, Batch index: 9, Loss: 0.6698595285415649\n",
      "Epoch: 2, Batch index: 10, Loss: 0.6661300659179688\n",
      "Epoch: 2, Batch index: 11, Loss: 0.6710147857666016\n",
      "Epoch: 2, Batch index: 12, Loss: 0.6565536856651306\n",
      "Epoch: 2, Batch index: 13, Loss: 0.6775041222572327\n",
      "Epoch: 2, Batch index: 14, Loss: 0.6510521769523621\n",
      "Epoch: 2, Batch index: 15, Loss: 0.6576119661331177\n",
      "Epoch: 2, Batch index: 16, Loss: 0.6757625937461853\n",
      "Epoch: 2, Batch index: 17, Loss: 0.6660497784614563\n",
      "Epoch: 2, Train accuracy: 0.657940685749054, Validation accuracy: 0.625\n",
      "Epoch: 3, Batch index: 0, Loss: 0.6722856163978577\n",
      "Epoch: 3, Batch index: 1, Loss: 0.6685598492622375\n",
      "Epoch: 3, Batch index: 2, Loss: 0.673579216003418\n",
      "Epoch: 3, Batch index: 3, Loss: 0.6841888427734375\n",
      "Epoch: 3, Batch index: 4, Loss: 0.6660167574882507\n",
      "Epoch: 3, Batch index: 5, Loss: 0.6775619387626648\n",
      "Epoch: 3, Batch index: 6, Loss: 0.6548237800598145\n",
      "Epoch: 3, Batch index: 7, Loss: 0.6768085360527039\n",
      "Epoch: 3, Batch index: 8, Loss: 0.6573607921600342\n",
      "Epoch: 3, Batch index: 9, Loss: 0.6813772916793823\n",
      "Epoch: 3, Batch index: 10, Loss: 0.6516615152359009\n",
      "Epoch: 3, Batch index: 11, Loss: 0.6662713289260864\n",
      "Epoch: 3, Batch index: 12, Loss: 0.6604104042053223\n",
      "Epoch: 3, Batch index: 13, Loss: 0.6569968461990356\n",
      "Epoch: 3, Batch index: 14, Loss: 0.6506423950195312\n",
      "Epoch: 3, Batch index: 15, Loss: 0.6714740991592407\n",
      "Epoch: 3, Batch index: 16, Loss: 0.6417739391326904\n",
      "Epoch: 3, Batch index: 17, Loss: 0.6603878140449524\n",
      "Epoch: 3, Train accuracy: 0.6719022393226624, Validation accuracy: 0.640625\n",
      "Epoch: 4, Batch index: 0, Loss: 0.6721219420433044\n",
      "Epoch: 4, Batch index: 1, Loss: 0.659416139125824\n",
      "Epoch: 4, Batch index: 2, Loss: 0.6582589149475098\n",
      "Epoch: 4, Batch index: 3, Loss: 0.6591305136680603\n",
      "Epoch: 4, Batch index: 4, Loss: 0.6709913015365601\n",
      "Epoch: 4, Batch index: 5, Loss: 0.6616871356964111\n",
      "Epoch: 4, Batch index: 6, Loss: 0.6582944989204407\n",
      "Epoch: 4, Batch index: 7, Loss: 0.6588773727416992\n",
      "Epoch: 4, Batch index: 8, Loss: 0.6728914380073547\n",
      "Epoch: 4, Batch index: 9, Loss: 0.6608836650848389\n",
      "Epoch: 4, Batch index: 10, Loss: 0.6569559574127197\n",
      "Epoch: 4, Batch index: 11, Loss: 0.6592535376548767\n",
      "Epoch: 4, Batch index: 12, Loss: 0.653987467288971\n",
      "Epoch: 4, Batch index: 13, Loss: 0.661566972732544\n",
      "Epoch: 4, Batch index: 14, Loss: 0.6518170237541199\n",
      "Epoch: 4, Batch index: 15, Loss: 0.6522912383079529\n",
      "Epoch: 4, Batch index: 16, Loss: 0.6551830172538757\n",
      "Epoch: 4, Batch index: 17, Loss: 0.672605574131012\n",
      "Epoch: 4, Train accuracy: 0.6945898532867432, Validation accuracy: 0.671875\n",
      "Epoch: 5, Batch index: 0, Loss: 0.650488555431366\n",
      "Epoch: 5, Batch index: 1, Loss: 0.6619141697883606\n",
      "Epoch: 5, Batch index: 2, Loss: 0.6541017293930054\n",
      "Epoch: 5, Batch index: 3, Loss: 0.6584322452545166\n",
      "Epoch: 5, Batch index: 4, Loss: 0.6696987748146057\n",
      "Epoch: 5, Batch index: 5, Loss: 0.6476296782493591\n",
      "Epoch: 5, Batch index: 6, Loss: 0.6541382074356079\n",
      "Epoch: 5, Batch index: 7, Loss: 0.6535160541534424\n",
      "Epoch: 5, Batch index: 8, Loss: 0.6604388952255249\n",
      "Epoch: 5, Batch index: 9, Loss: 0.6528723835945129\n",
      "Epoch: 5, Batch index: 10, Loss: 0.6594895124435425\n",
      "Epoch: 5, Batch index: 11, Loss: 0.6533790826797485\n",
      "Epoch: 5, Batch index: 12, Loss: 0.6562212705612183\n",
      "Epoch: 5, Batch index: 13, Loss: 0.6597392559051514\n",
      "Epoch: 5, Batch index: 14, Loss: 0.6544495820999146\n",
      "Epoch: 5, Batch index: 15, Loss: 0.6580278277397156\n",
      "Epoch: 5, Batch index: 16, Loss: 0.6472651362419128\n",
      "Epoch: 5, Batch index: 17, Loss: 0.6636856198310852\n",
      "Epoch: 5, Train accuracy: 0.717277467250824, Validation accuracy: 0.6875\n",
      "Epoch: 6, Batch index: 0, Loss: 0.6597655415534973\n",
      "Epoch: 6, Batch index: 1, Loss: 0.6606478691101074\n",
      "Epoch: 6, Batch index: 2, Loss: 0.6406412720680237\n",
      "Epoch: 6, Batch index: 3, Loss: 0.633506715297699\n",
      "Epoch: 6, Batch index: 4, Loss: 0.652040421962738\n",
      "Epoch: 6, Batch index: 5, Loss: 0.6542466282844543\n",
      "Epoch: 6, Batch index: 6, Loss: 0.6676910519599915\n",
      "Epoch: 6, Batch index: 7, Loss: 0.6536778807640076\n",
      "Epoch: 6, Batch index: 8, Loss: 0.6655691862106323\n",
      "Epoch: 6, Batch index: 9, Loss: 0.6525819301605225\n",
      "Epoch: 6, Batch index: 10, Loss: 0.6588234305381775\n",
      "Epoch: 6, Batch index: 11, Loss: 0.6344115734100342\n",
      "Epoch: 6, Batch index: 12, Loss: 0.6555343866348267\n",
      "Epoch: 6, Batch index: 13, Loss: 0.6438243985176086\n",
      "Epoch: 6, Batch index: 14, Loss: 0.6503654718399048\n",
      "Epoch: 6, Batch index: 15, Loss: 0.6443921327590942\n",
      "Epoch: 6, Batch index: 16, Loss: 0.6499142646789551\n",
      "Epoch: 6, Batch index: 17, Loss: 0.6504853367805481\n",
      "Epoch: 6, Train accuracy: 0.7312390804290771, Validation accuracy: 0.6875\n",
      "Epoch: 7, Batch index: 0, Loss: 0.6346762776374817\n",
      "Epoch: 7, Batch index: 1, Loss: 0.6571516990661621\n",
      "Epoch: 7, Batch index: 2, Loss: 0.6308640241622925\n",
      "Epoch: 7, Batch index: 3, Loss: 0.6461473703384399\n",
      "Epoch: 7, Batch index: 4, Loss: 0.6521977782249451\n",
      "Epoch: 7, Batch index: 5, Loss: 0.6547255516052246\n",
      "Epoch: 7, Batch index: 6, Loss: 0.6443697214126587\n",
      "Epoch: 7, Batch index: 7, Loss: 0.635634183883667\n",
      "Epoch: 7, Batch index: 8, Loss: 0.6427600383758545\n",
      "Epoch: 7, Batch index: 9, Loss: 0.6556325554847717\n",
      "Epoch: 7, Batch index: 10, Loss: 0.6511054635047913\n",
      "Epoch: 7, Batch index: 11, Loss: 0.6688481569290161\n",
      "Epoch: 7, Batch index: 12, Loss: 0.639672040939331\n",
      "Epoch: 7, Batch index: 13, Loss: 0.6397107839584351\n",
      "Epoch: 7, Batch index: 14, Loss: 0.643266499042511\n",
      "Epoch: 7, Batch index: 15, Loss: 0.646412193775177\n",
      "Epoch: 7, Batch index: 16, Loss: 0.6476576924324036\n",
      "Epoch: 7, Batch index: 17, Loss: 0.6479086875915527\n",
      "Epoch: 7, Train accuracy: 0.7591623067855835, Validation accuracy: 0.6875\n",
      "Epoch: 8, Batch index: 0, Loss: 0.6505336165428162\n",
      "Epoch: 8, Batch index: 1, Loss: 0.6363998651504517\n",
      "Epoch: 8, Batch index: 2, Loss: 0.6521551609039307\n",
      "Epoch: 8, Batch index: 3, Loss: 0.6386851668357849\n",
      "Epoch: 8, Batch index: 4, Loss: 0.637184739112854\n",
      "Epoch: 8, Batch index: 5, Loss: 0.6472139954566956\n",
      "Epoch: 8, Batch index: 6, Loss: 0.6435099840164185\n",
      "Epoch: 8, Batch index: 7, Loss: 0.6406227350234985\n",
      "Epoch: 8, Batch index: 8, Loss: 0.6388619542121887\n",
      "Epoch: 8, Batch index: 9, Loss: 0.6227021813392639\n",
      "Epoch: 8, Batch index: 10, Loss: 0.6375263333320618\n",
      "Epoch: 8, Batch index: 11, Loss: 0.6504854559898376\n",
      "Epoch: 8, Batch index: 12, Loss: 0.6460011601448059\n",
      "Epoch: 8, Batch index: 13, Loss: 0.6460970640182495\n",
      "Epoch: 8, Batch index: 14, Loss: 0.6412132978439331\n",
      "Epoch: 8, Batch index: 15, Loss: 0.6428282856941223\n",
      "Epoch: 8, Batch index: 16, Loss: 0.640597403049469\n",
      "Epoch: 8, Batch index: 17, Loss: 0.6294741034507751\n",
      "Epoch: 8, Train accuracy: 0.7905759215354919, Validation accuracy: 0.71875\n",
      "Epoch: 9, Batch index: 0, Loss: 0.6260685920715332\n",
      "Epoch: 9, Batch index: 1, Loss: 0.6308558583259583\n",
      "Epoch: 9, Batch index: 2, Loss: 0.6447997093200684\n",
      "Epoch: 9, Batch index: 3, Loss: 0.6365520358085632\n",
      "Epoch: 9, Batch index: 4, Loss: 0.6353809833526611\n",
      "Epoch: 9, Batch index: 5, Loss: 0.6237306594848633\n",
      "Epoch: 9, Batch index: 6, Loss: 0.6502087712287903\n",
      "Epoch: 9, Batch index: 7, Loss: 0.6509247422218323\n",
      "Epoch: 9, Batch index: 8, Loss: 0.6360477209091187\n",
      "Epoch: 9, Batch index: 9, Loss: 0.6329430341720581\n",
      "Epoch: 9, Batch index: 10, Loss: 0.6417502164840698\n",
      "Epoch: 9, Batch index: 11, Loss: 0.6493040919303894\n",
      "Epoch: 9, Batch index: 12, Loss: 0.6351355910301208\n",
      "Epoch: 9, Batch index: 13, Loss: 0.6331406235694885\n",
      "Epoch: 9, Batch index: 14, Loss: 0.6324130296707153\n",
      "Epoch: 9, Batch index: 15, Loss: 0.6191244721412659\n",
      "Epoch: 9, Batch index: 16, Loss: 0.637117326259613\n",
      "Epoch: 9, Batch index: 17, Loss: 0.6234995126724243\n",
      "Epoch: 9, Train accuracy: 0.8289703130722046, Validation accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "\n",
    "torch.manual_seed(1)\n",
    "#import SGD optimizer\n",
    "model = PytorchMLP(2, 2)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "epochs= 10\n",
    "\n",
    "\n",
    "for epoch in range(epochs): \n",
    "    model= model.train()\n",
    "    \n",
    "    for batch_idx, (features ,labels) in enumerate(train_loader):\n",
    "        logits = model(features)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch: {epoch}, Batch index: {batch_idx}, Loss: {loss.item()}\")\n",
    "    train_accuracy = compute_accuracy(model, train_loader)\n",
    "    val_accuracy = compute_accuracy(model, validatation_loader)\n",
    "    print(f\"Epoch: {epoch}, Train accuracy: {train_accuracy}, Validation accuracy: {val_accuracy}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.8289703130722046, Validation accuracy: 0.75, Test accuracy: 0.7876105904579163\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = compute_accuracy(model, train_loader)\n",
    "val_accuracy = compute_accuracy(model, validatation_loader)\n",
    "test_accuracy = compute_accuracy(model, test_loader)\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy}, Validation accuracy: {val_accuracy}, Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
